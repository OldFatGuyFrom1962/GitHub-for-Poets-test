{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OldFatGuyFrom1962/GitHub-for-Poets-test/blob/main/31_NLP_Handson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Hands-on\n",
        "\n",
        "This notebook is available at https://bit.ly/aiq-nlp .\n",
        "\n",
        "## Start here\n",
        "\n",
        "To get things started, execute this notebook. The steps required are:\n",
        "- If you'd like to save changes you make, in the top menu, select \"File\" --> \"Save copy in ...\" (select option that applies to you).\n",
        "- In the top menu, select \"Runtime\" --> \"Change runtime type\" --> \"Hardware accelerator\" = \"GPU\".\n",
        "- Button on top right, press \"Connect\".\n",
        "- In menu top menu, \"Runtime\" --> \"Run all\".\n",
        "\n",
        "The execution of the notebook will take a few minutes the first time it is run as several datasets, some software, and a model need to be downloaded.\n",
        "\n",
        "This notebook demonstrates course concepts in the context of natural language processing (NLP). The following sections include:\n",
        "\n",
        "## Setup\n",
        "\n",
        "Installing, downloading, and importing required packages and data.\n",
        "\n",
        "## Sentiment Model\n",
        "\n",
        "Downloads and sets up a sentiment model from Huggingface.\n",
        "  \n",
        "  - Data pre-processing: how to process data for use in the model.\n",
        "  - Running the model: demonstration of how to run the model.\n",
        "  - Embedding Space: visualization of the model's token embeddings.\n",
        "  - Performance: measuring accuracy of the model on a sample rotten tomatoes dataset.\n",
        "  - Trulens: Model Wrapper: how to wrap the model for use with Trulens.\n",
        "\n",
        "## Attributions\n",
        "\n",
        "Basic demonstration of attributions using Trulens.\n",
        "  \n",
        "  - Baselines: demonstration of adjusting the baseline used for Trulens attributions.\n",
        "\n",
        "## Fairness\n",
        "\n",
        "Finding inputs which demonstrate model unfairness.\n",
        "\n",
        "  - Gender in embedding space: visualization of the embedding space's gender direction.\n",
        "\n",
        "## Drift\n",
        "\n",
        "Demonstration of different options for measuring drift.\n",
        "\n",
        "  - Model score drift: drift in model score, same as in tabular data.\n",
        "  - Token distribution drift: comparing distributions of tokens across datasets.\n",
        "  - Embedding distribution drift: comparing distributions in the embedding space.\n",
        "  - Gender distribution drift: comparing distributions in embedding space using an extracted gender dimension.\n",
        "\n",
        "# Interactive widgets\n",
        "\n",
        "There are widgets throughout this notebook where you can adjust inputs or inspect details to/on various demonstrations. These are colored in a thick <span style=\"border: 5px solid teal;\">teal</span> border.\n",
        "\n",
        "# Starting up\n",
        "\n",
        "Once you evaluate the entire notebook, you can skip any section starting with `Skip`, and focus on sections starting with `Playground`.\n",
        "\n",
        "# Homework\n",
        "\n",
        "Adapt the appropriate [Trulens](https://trulens.org) quickstart notebook to your favorite model / dataset. The available quickstarts are:\n",
        "\n",
        "* [vision with pytorch](https://colab.research.google.com/drive/1n77IGrPDO2XpeIVo_LQW0gY78enV-tY9?usp=sharing)\n",
        "* [vision with tensorflow](https://colab.research.google.com/drive/1f-ETsdlppODJGQCdMXG-jmGmfyWyW2VD?usp=sharing)\n",
        "* [NLP with pytorch](https://colab.research.google.com/drive/18GcjsYMkRbxPDDS3J6BEbKnb7AY-1-Wa?usp=sharing)\n",
        "* [NLP with tensorflow](https://colab.research.google.com/drive/1K09IvN7cMTkzsnb-uAeA0YQNfDU7Ibhs?usp=sharing)\n",
        "\n",
        "## Optional Adventure\n",
        "\n",
        "Replace elements of this notebook with your favorite ... (in order of increasing difficulty):\n",
        "\n",
        "- Sentiment classification dataset. Code portions that will have to change are marked with `DATA`.\n",
        "- Non-sentiment classification dataset. Marked with `DATA`.\n",
        "    - more useful if model updated too\n",
        "- Huggingface classification model. Marked with `HUGS`.\n",
        "- Huggingface non-classification model. Marked with `CLASS`.\n",
        "    - more useful if you have appropriate data too\n",
        "\n",
        "While Trulens supports Tensorflow, most of this notebook is tailored to pytorch so we do not recommend trying to use it with tensorflow."
      ],
      "metadata": {
        "id": "JFNHcCCGNKy8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyqRPOXI8cmt",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Skip: Setup\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "\n",
        "# Install requirements.\n",
        "while True:\n",
        "  try:\n",
        "    import trulens\n",
        "    import lzma\n",
        "    import pickle\n",
        "    import datasets\n",
        "    import domonic\n",
        "    import gdown\n",
        "    from openTSNE import TSNE\n",
        "    import torch\n",
        "    import transformers\n",
        "  except Exception:\n",
        "    ! {sys.executable} -m pip install git+https://github.com/truera/trulens.git@piotrm/aiq-nlp\n",
        "    ! {sys.executable} -m pip install transformers datasets openTSNE domonic==0.9.8 gdown torch\n",
        "  else:\n",
        "    break\n",
        "\n",
        "import base64\n",
        "import functools\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "from pathlib import Path\n",
        "import re\n",
        "from typing import Callable, Dict, List, Tuple, Sequence\n",
        "\n",
        "from datasets import load_dataset\n",
        "from IPython.display import clear_output\n",
        "from IPython.display import display\n",
        "from ipywidgets import interact\n",
        "from ipywidgets import interactive\n",
        "from ipywidgets import widgets\n",
        "import numpy as np\n",
        "import numpy.typing as npt\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "import transformers as hugs\n",
        "\n",
        "from trulens.nn.attribution import Cut\n",
        "from trulens.nn.attribution import IntegratedGradients\n",
        "from trulens.nn.attribution import OutputCut\n",
        "from trulens.nn.models import get_model_wrapper\n",
        "from trulens.nn.quantities import ClassQoI\n",
        "from trulens.utils.nlp import token_baseline\n",
        "from trulens.utils.typing import ModelInputs\n",
        "\n",
        "# need to use old ipywidgets:\n",
        "# ! {sys.executable} -m pip install ipywidgets==7.7.1\n",
        "#try:\n",
        "#  from google.colab import output\n",
        "#  output.enable_custom_widget_manager()\n",
        "\n",
        "# Figure = go.FigureWidget # use this if running in vscode\n",
        "Figure = go.Figure # use this if running in google colab or jupyter\n",
        "\n",
        "# Download some pre-computed data.\n",
        "if not Path(\"tsne_embedding.lzma\").exists():\n",
        "  gdown.download(\n",
        "    \"https://drive.google.com/file/d/1ZA8jyv026Q7T1RCJFtxxfCUl1JHXNFVP/view?usp=sharing\",\n",
        "    fuzzy=True, resume=True\n",
        ")\n",
        "\n",
        "try:\n",
        "  # DATA: Preload datasets. More about them later.\n",
        "  rotten_train = load_dataset(\"rotten_tomatoes\", split=\"train\")\n",
        "  rotten_test = load_dataset(\"rotten_tomatoes\", split=\"test\")\n",
        "  rotten_texts = list(rotten_train['text']) + list(rotten_test['text'])\n",
        "  \n",
        "  imdb_train = load_dataset(\"imdb\", \"plain_text\", split=\"train\")\n",
        "  imdb_test = load_dataset(\"imdb\", \"plain_text\", split=\"test\")\n",
        "  imdb_texts = list(imdb_train['text']) + list(imdb_test['text'])\n",
        "\n",
        "except Exception as e:\n",
        "  print(\"WARNING: could not load huggingface datasets, will use blank replacements\")\n",
        "  print(str(e))\n",
        "\n",
        "  rotten_texts = [\"fake rotten tomatoes sentence; person woman man camera TV\"] * 10\n",
        "  rotten_train = dict(text=rotten_texts, label=[0] * 10)\n",
        "  rotten_test = dict(text=rotten_texts, label=[0] * 10)\n",
        "\n",
        "  imdb_texts = [\"fake imdb sentence; person woman man camera TV\"] * 10\n",
        "  imdb_train = dict(text=imdb_texts, label=[0] * 10)\n",
        "  imdb_test = dict(text=imdb_texts, label=[0] * 10)\n",
        "\n",
        "try:\n",
        "  # DATA: extra dataset for playing around\n",
        "  tweets_file = Path(\"training.1600000.processed.noemoticon.csv\")\n",
        "  if not tweets_file.exists():\n",
        "      ! wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
        "      ! unzip trainingandtestdata.zip\n",
        "\n",
        "  tweets = pd.read_csv(tweets_file, encoding='ISO-8859-1', header=None, names=[\"polarity\", \"id\", \"timestamp\", \"query\", \"user\", \"text\"])\n",
        "  tweet_texts = list(tweets['text'])\n",
        "\n",
        "except Exception as e:\n",
        "  print(\"WARNING: could not load tweet dataset, will use blank replacements\")\n",
        "  print(str(e))\n",
        "\n",
        "  tweet_texts = [\"fake tweet sentence; person woman man camera TV\"] * 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMf8BfYp8cmu"
      },
      "source": [
        "# Sentiment Classification Model\n",
        "\n",
        "[Huggingface](https://huggingface.co/models) offers a variety of pre-trained NLP models to explore. We exemplify in this notebook a [transformer-based twitter sentiment classification model](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
        "\n",
        "In the below cell, we point out, with `HUGS`, elements that you would need to update to replace the given model with another hugging face model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4UOObDW8cmu",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Huggingface NLP model setup\n",
        "\n",
        "\n",
        "# Wrap all of the components needed to run a model.\n",
        "class Model:\n",
        "    # device = torch.device(\"cpu\", 0)\n",
        "    # Can also use cuda if available:\n",
        "    device = torch.device(\"cuda\", 0)\n",
        "\n",
        "    # HUGS: model name, see https://huggingface.co/models for others\n",
        "    # https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english\n",
        "    MODEL = f\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "    model: hugs.PreTrainedModel = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL\n",
        "    ).to(device)\n",
        "\n",
        "    tokenizer: hugs.PreTrainedTokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "\n",
        "    # HUGS: the embeddings vectors, one for each token\n",
        "    embeddings: npt.NDArray[np.float32] = \\\n",
        "        model.distilbert.embeddings.word_embeddings.weight.detach().cpu().numpy()\n",
        "\n",
        "    # HUGS: name of the layer that produces token embeddings. The trulens\n",
        "    # wrapping cell later in this notebook can be helpful in figuring out this\n",
        "    # parameter.\n",
        "    embeddings_layer: str = 'distilbert_embeddings_word_embeddings'\n",
        "\n",
        "    # number of dimensions in token embedding\n",
        "    embedding_size: int = embeddings.shape[1]\n",
        "\n",
        "    # HUGS: maximum number of tokens to send to model\n",
        "    max_length: int = 128\n",
        "\n",
        "    # HUGS: Maximum number of instances we can evaluate the model on at once. This is\n",
        "    # necessary when using a GPU with a limited amount of memory.\n",
        "    rebatch_size: int = 16\n",
        "\n",
        "    id_of_token: Dict[str, int] = tokenizer.get_vocab()\n",
        "    token_of_id: Dict[int, str] = {v: k for k, v in id_of_token.items()}\n",
        "\n",
        "    # number of tokens in vocabulary\n",
        "    vocab_size: int = len(id_of_token)\n",
        "\n",
        "    def _vocab(token_of_id, vocab_size):\n",
        "        # Python list comprehension scoping workaround\n",
        "        return np.array([token_of_id[i] for i in range(vocab_size)])\n",
        "\n",
        "    # tokens in order\n",
        "    vocab: npt.NDArray[str] = _vocab(token_of_id, vocab_size)\n",
        "\n",
        "    # CLASS\n",
        "    labels = ['negative', 'positive']\n",
        "\n",
        "    # CLASS\n",
        "    NEGATIVE: int = labels.index('negative')\n",
        "    POSITIVE: int = labels.index('positive')\n",
        "\n",
        "    def tokenize(texts: List[str]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Tokenize a list of `texts` into a form appropriate for `TwitterSentiment.model` .\n",
        "        \"\"\"\n",
        "        return Model.tokenizer(\n",
        "            texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=Model.max_length,\n",
        "            return_tensors='pt'\n",
        "        ).to(Model.device)\n",
        "\n",
        "    # CLASS\n",
        "    def evaluate_to_logits(\n",
        "        texts: List[str], batch_size=rebatch_size\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Evaluate a collection of `texts` into their logits scores.\n",
        "        \"\"\"\n",
        "\n",
        "        logits = []\n",
        "\n",
        "        inputs = Model.tokenize(texts)\n",
        "\n",
        "        for idx in tqdm(range(0, len(texts), batch_size),\n",
        "                        desc=\"evaluating model\"):\n",
        "\n",
        "            batch_logits = Model.model(\n",
        "                input_ids=inputs['input_ids'][idx:idx + batch_size],\n",
        "                attention_mask=inputs['attention_mask'][idx:idx + batch_size]\n",
        "            ).logits\n",
        "\n",
        "            logits.append(batch_logits.detach())\n",
        "\n",
        "        return torch.concat(logits).detach().cpu()\n",
        "\n",
        "    # CLASS\n",
        "    def evaluate_to_probits(\n",
        "        texts: List[str], batch_size=rebatch_size\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Evaluate a collection of `texts` into their probits scores.\n",
        "        \"\"\"\n",
        "\n",
        "        logits = Model.evaluate_to_logits(\n",
        "            texts, batch_size=batch_size\n",
        "        )\n",
        "        return torch.nn.functional.softmax(logits, dim=1).detach().cpu()\n",
        "\n",
        "    # HUGS\n",
        "    def token_str(token_id: int) -> str:\n",
        "        \"\"\"\n",
        "        Given a `token_id`, produce a string of how it should be drawn.\n",
        "        \"\"\"\n",
        "        tok = Model.tokenizer.decode(token_id)\n",
        "        if tok.startswith(\"##\"):\n",
        "            # token starts with \"##\" to denote a word postfix\n",
        "            return tok[2:]\n",
        "        else:\n",
        "            # if not a postfix, add space better indicate a complete word\n",
        "            # separation\n",
        "            return \" \" + tok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkxHugWm8cmv"
      },
      "source": [
        "## Data pre-processing\n",
        "\n",
        "This section demonstrates the initial steps of an NLP model evaluation, the tokenization and conversion to embeddings."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Skip: Details\n",
        "\n",
        "# Utilities we will use for interactive parts of this notebook. Please ignore.\n",
        "aiq_layout = dict(\n",
        "    border=\"10px solid teal\", padding=\"10px\", width=\"100%\", margin=\"0px\"\n",
        ")\n",
        "\n",
        "# Interaction utilities.\n",
        "textbox = (\n",
        "    lambda t=\n",
        "    \"I'm a sentence. The last part of this sentence is not a real wordle.\", c=\n",
        "    True, d=\"input\": widgets.Text(\n",
        "        value=t,\n",
        "        continuous_update=c,\n",
        "        layout=aiq_layout,\n",
        "        description=d + (\" (enter to update)\" if not c else \"\"),\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "r_bla-xfyZ4x",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvFb7aUt8cmv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Playground: Input processing\n",
        "#@markdown\n",
        "#@markdown Enter a piece of text in the widget below to see the tokenized version in the cell output. There you can see: \n",
        "#@markdown - \"INPUT TEXT\" -- the piece of text you entered.\n",
        "#@markdown - \"MODEL INPUTS\" -- the data structure provided to the model that represents various aspects of the input text.\n",
        "#@markdown - \"TOKENS\" -- string representations of the tokens making up the input text.\n",
        "#@markdown - \"EMBEDDINGS\" -- the embedding representation of the tokens.\n",
        "\n",
        "@interact(text=textbox())\n",
        "def show_parse(text: str):\n",
        "\n",
        "    print(\"INPUT TEXT\\n\", text, \"\\n\")\n",
        "\n",
        "    # Input sentences need to be tokenized first.\n",
        "    inputs = Model.tokenize([text])\n",
        "\n",
        "    # The tokenizer gives us vocabulary indexes for each input token (in this case,\n",
        "    # words and some word parts like the \"'m\" part of \"I'm\" are tokens).\n",
        "\n",
        "    print(\"MODEL INPUTS\\n\", inputs, \"\\n\")\n",
        "\n",
        "    # HUGS: Decode helps inspecting the tokenization produced:\n",
        "    tokens = Model.tokenizer.batch_decode(torch.flatten(inputs['input_ids']))\n",
        "\n",
        "    # Normally decode would give us a single string for each sentence but we would\n",
        "    # not be able to see some of the non-word tokens there. Flattening first gives\n",
        "    # us a string for each input_id.\n",
        "\n",
        "    print(\"TOKENS\\n\", tokens, \"\\n\")\n",
        "\n",
        "    # HUGS: Each token is represented by a dense vector in the model.\n",
        "    toks = inputs['input_ids'].detach().cpu().numpy()\n",
        "    embs = np.array([Model.embeddings[token_id] for token_id in toks])[0]\n",
        "\n",
        "    print(\"EMBEDDINGS\\n\", embs, \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk2_xQVs8cmw",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Playground: Model evaluation\n",
        "#@markdown Enter a piece of text to evaluate the sentiment model on. In the cell output you should see:\n",
        "#@markdown - The classification label (negative or positive).\n",
        "#@markdown - The classification scores (two real numbers, one for each class).\n",
        "#@markdown - The input text evaluated.\n",
        "\n",
        "model_results = []\n",
        "\n",
        "\n",
        "@interact(text=textbox())\n",
        "def show_output(text):\n",
        "    global model_results\n",
        "    results = model_results\n",
        "\n",
        "    # Get the model appropriate inputs from a single text instance:\n",
        "    inputs = Model.tokenize([text])\n",
        "\n",
        "    # Run the model on it:\n",
        "    outputs = Model.model(**inputs)\n",
        "\n",
        "    # CLASS: From logits we can extract the most likely class for each sentence and its\n",
        "    # readable label.\n",
        "    predictions = [Model.labels[i] for i in outputs.logits.argmax(axis=1)]\n",
        "\n",
        "    # CLASS\n",
        "    results.insert(\n",
        "        0, (predictions[0], outputs.logits.detach().cpu().numpy()[0], text)\n",
        "    )\n",
        "    results = results[0:10]\n",
        "\n",
        "    for result in results:\n",
        "        print(*result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSzU71Py8cmw"
      },
      "source": [
        "## Embedding Space\n",
        "\n",
        "This section visualizes the model's embedding space. It is based on TSNE dimensionality reduction that reduces 768 dimensional embedding vectors into just 2 dimensions. Ideally tokens that are nearby in the original space should show up nearby in the visualization but this naturally not exact. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY9WCPPb8cmw",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Skip: Details\n",
        "\n",
        "# AIQ: The following computation takes too long on colab. The results should\n",
        "# have been downloaded for you earlier in this notebook.\n",
        "\n",
        "# HUGS: If using a different model, you might have to recompute this or just skip this step.\n",
        "tsne_filename = Path(\"tsne_embedding.lzma\")\n",
        "if tsne_filename.exists():\n",
        "    print(\"loading dimensionality reduction\")\n",
        "    with lzma.open(tsne_filename, mode='rb') as fh:\n",
        "        tsne_embedding = pickle.load(fh)\n",
        "\n",
        "else:\n",
        "    print(\n",
        "        \"computing, if you are running this in colab, be prepared to wait a long time\"\n",
        "    )\n",
        "    man = TSNE(\n",
        "        n_jobs=mp.cpu_count(),\n",
        "        verbose=True,\n",
        "        n_iter=10000,\n",
        "        learning_rate=200,\n",
        "        negative_gradient_method='bh',\n",
        "        metric=\"cosine\"\n",
        "    )\n",
        "    tsne_embedding = man.fit(Model.embeddings)\n",
        "\n",
        "    print(\"saving\")\n",
        "    with lzma.open(tsne_filename, mode='wb') as fh:\n",
        "        pickle.dump(obj=tsne_embedding, file=fh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVmxbI-b8cmx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Playground: Embedding space\n",
        "#@markdown In this cell you should see a visualization of token embeddings. As they are high-dimensional vectors, they have been reduced to 2 dimensional points via a dimensionality reduction technique. While a lot of information about a token is lost in this visualization, some higher level patterns can be seen:\n",
        "#@markdown - Long strings of points tend to represent tokens that have some notion of order. For example the tokens representing years 1831 through 2018 seen in the lower right portion of the figure and tokens \"1st\" through \"19th\" right above the year sequence.\n",
        "#@markdown - Pairs or small groups of related or semantically equivalent tokens make up the bulk of the visualization. For example \"presently\" next to \"nowadays\".\n",
        "#@markdown - Large clumps of special tokens or rarer tokens from non-English languages like the large clump in the upper left that contains many Japanese or Chinese characters.\n",
        "\n",
        "# AIQ: This is computationally intensive picture.\n",
        "\n",
        "plotly_layout = dict(paper_bgcolor=\"teal\", margin=dict(l=10, r=10, t=10, b=10))\n",
        "\n",
        "fig = Figure(layout=dict(width=800, height=800, **plotly_layout))\n",
        "fig.update_xaxes(\n",
        "    showticklabels=False\n",
        ")\n",
        "fig.update_yaxes(\n",
        "    showticklabels=False\n",
        ")\n",
        "fig.add_scatter(\n",
        "    x=tsne_embedding[:, 0],\n",
        "    y=tsne_embedding[:, 1],\n",
        "    text=Model.vocab[:],\n",
        "    mode='markers',\n",
        "    marker_size=2\n",
        ")\n",
        "\n",
        "display(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_gvi71u8cmx"
      },
      "source": [
        "## Performance\n",
        "\n",
        "We load a [rotten tomatoes movie review sentiment dataset](https://huggingface.co/datasets/rotten_tomatoes) as the first source of data. Later in the drift section we will add a different sentiment dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAHkKXqH8cmx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Rotten tomatoes dataset\n",
        "\n",
        "# DATA: https://huggingface.co/datasets/rotten_tomatoes\n",
        "rotten_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL-oiXGf8cmx"
      },
      "outputs": [],
      "source": [
        "#@title Accuracy measurement and accuracy on rotten tomatoes\n",
        "\n",
        "def accuracy(X: npt.NDArray[float], Y_true: npt.NDArray[int]) -> float:\n",
        "    \"\"\"\n",
        "    Determine model accuracy on the given dataset `X` with ground truth labels\n",
        "    `Y_true`. If this is running slowly, you might be running without GPU. \n",
        "    \"\"\"\n",
        "    \n",
        "    # CLASS\n",
        "    Y_probits = Model.evaluate_to_probits(X).detach().cpu().numpy()\n",
        "\n",
        "    # CLASS\n",
        "    Y_pred = np.argmax(Y_probits, axis=1)\n",
        "    correct = Y_pred == Ytrue\n",
        "\n",
        "    return correct.mean()\n",
        "\n",
        "# DATA\n",
        "for dataset_name, X, Ytrue in [\n",
        "    (\"rotten train\", rotten_train['text'], rotten_train['label']),\n",
        "    (\"rotten test\", rotten_test['text'], rotten_test['label'])]:\n",
        "    print(dataset_name, f\"accuracy = {accuracy(X, Ytrue) * 100:0.2f} %\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfPwNFdG8cmx"
      },
      "source": [
        "## Trulens: Model Wrapper\n",
        "\n",
        "As in the prior notebooks, we need to wrap the pytorch model with the appropriate Trulens functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw79gFcW8cmx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Trulens wrapping\n",
        "\n",
        "# HUGS: Output might be useful for figuring out embedding layer for different models.\n",
        "Model.wrapper = get_model_wrapper(Model.model, device=Model.device)\n",
        "\n",
        "#Model.wrapper.print_layer_names()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzPEjNgy8cmy"
      },
      "source": [
        "# Explanations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhL_c8im8cmy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Trulens Integrated Gradients and Visualization setup\n",
        "\n",
        "# HUGS: Set up the attribution method, here it will be \"Integrated Gradients\"\n",
        "# CLASS: Will have to change qoi to potentially LambdaQoI.\n",
        "common_attributor_arguments = dict(\n",
        "    model=Model.wrapper,\n",
        "    resolution=128,\n",
        "    rebatch_size=32,\n",
        "    doi_cut=Cut(Model.embeddings_layer),\n",
        "    qoi=ClassQoI(Model.POSITIVE),\n",
        "    qoi_cut=OutputCut(accessor=lambda o: o['logits'])\n",
        ")\n",
        "# HUGS\n",
        "infl = IntegratedGradients(\n",
        "    **common_attributor_arguments\n",
        ")\n",
        "\n",
        "from trulens.visualizations import NLP\n",
        "\n",
        "# HUGS: Set up visualization utilities.\n",
        "V = NLP(\n",
        "    wrapper=Model.wrapper,\n",
        "    labels=Model.labels,\n",
        "    decode=Model.token_str,\n",
        "    tokenize=lambda sentences: ModelInputs(kwargs=Model.tokenize(sentences,)).map(lambda t: t.to(Model.device)),\n",
        "    # huggingface models can take as input the keyword args as per produced by\n",
        "    # their tokenizers.\n",
        "    input_accessor=lambda x: x.kwargs['input_ids'],\n",
        "    # for huggingface models, input/token ids are under input_ids key in the\n",
        "    # input dictionary\n",
        "    output_accessor=lambda x: x['logits'],\n",
        "    # and logits under 'logits' key in the output dictionary\n",
        "    hidden_tokens=set([Model.tokenizer.pad_token_id])\n",
        "    # do not display these tokens\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Playground: Attribution\n",
        "#@markdown Enter a piece of text in the box below and press enter to display the explanation of what about the input was most important on the output. The output rows include:\n",
        "#@markdown - The \"quantity of interest\" which indicates which apsect of model output is being explained, in this case it is the positive class score. This is indicated by both \"ClassQoI_1\" where 1 indicates the index of the positive score, and the teal square around the name of that classs, \"positive\".\n",
        "#@markdown - The classification outcome (white rectangle around the class name). The relative scores of each class is indicated by the green bars above each class label.\n",
        "#@markdown - Influnece of each token (indicated by positive/green or negative/red contributions) to the quantity of interest.\n",
        "\n",
        "results = []\n",
        "\n",
        "@interact(text=textbox(c=False))\n",
        "def show_attribution(text):\n",
        "    global results\n",
        "\n",
        "    # Token attribution visualization takes in a list of sentences and the\n",
        "    # attribution method to compute the attributions.\n",
        "    token_attribution = V.tokens([text], infl) \n",
        "\n",
        "    results.insert(0, token_attribution)\n",
        "    results = results[:10]\n",
        "\n",
        "    for result in results:\n",
        "        display(result)"
      ],
      "metadata": {
        "id": "U_H074esy9j1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmVXexNM8cmy"
      },
      "source": [
        "## Baselines\n",
        "\n",
        "We see in the above results that special tokens such as the sentence end **&lt;/s&gt;** contributes are found to contribute a lot to the model outputs. While this may be useful in some contexts, we are more interested in the contributions of the actual words in these sentences. To focus on the words more, we need to adjust the **baseline** used in the integrated gradients computation. By default in the instantiation so far, the baseline for each token is a zero vector of the same shape as its embedding. By making the basaeline be identicaly to the explained instances on special tokens, we can rid their impact from our measurement. Trulens provides a utility for this purpose in terms of `token_baseline` which constructs for you the methods to compute the appropriate baseline. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxNN3f0T8cmy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Baseline setup\n",
        "\n",
        "# HUGS\n",
        "inputs_baseline_ids, inputs_baseline_embeddings = token_baseline(\n",
        "    keep_tokens=set([Model.tokenizer.cls_token_id, Model.tokenizer.sep_token_id]),\n",
        "    # Which tokens to preserve.\n",
        "    replacement_token=Model.tokenizer.pad_token_id,\n",
        "\n",
        "    # AIQ: Try changing `replacement_token` parameter to other special or non\n",
        "    # special tokens.\n",
        "\n",
        "    # replacement_token=Model.tokenizer.mask_token_id,\n",
        "    # replacement_token=Model.tokenizer.vocab[\"happy\"],\n",
        "\n",
        "    # HUGS: What to replace tokens with.\n",
        "    input_accessor=lambda x: x.kwargs['input_ids'],\n",
        "    ids_to_embeddings=Model.model.get_input_embeddings()\n",
        "    # Callable to produce embeddings from token ids.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlRZmU_C8cmy"
      },
      "source": [
        "We can now inspect the baselines on some example sentences. The first method returned by `token_baseline` gives us token ids to inspect while the second gives us the embeddings of the baseline which we will pass to the attributions method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrOpj-hN8cmy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Playground: Attribution with Pad Baseline (default baseline on left, Pad on right)\n",
        "#@markdown The explanations of the given text are given with both \"default\" baselines (left) and baselines that replace non-special tokens with \"[PAD]\".\n",
        "#@markdown - Notice the influence of special tokens at the start and end of the input with the two different baselines.\n",
        "#@markdown\n",
        "#@markdown\n",
        "\n",
        "infl_positive_baseline = IntegratedGradients(\n",
        "    baseline=inputs_baseline_embeddings, **common_attributor_arguments\n",
        ")\n",
        "\n",
        "results2 = []\n",
        "\n",
        "\n",
        "@interact(text=textbox(c=False))\n",
        "def show_attribution(text):\n",
        "    global results2\n",
        "\n",
        "    default_result = widgets.HTML(V.tokens([text], infl).data)\n",
        "    baseline_result = widgets.HTML(\n",
        "        V.tokens([text], infl_positive_baseline).data\n",
        "    )\n",
        "\n",
        "    results2.insert(0, (default_result, baseline_result))\n",
        "    results2 = results2[:3]\n",
        "\n",
        "    parts = []\n",
        "\n",
        "    for result in results2:\n",
        "        parts.append(widgets.HBox(result))\n",
        "\n",
        "    display(widgets.VBox(parts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2x8vTQe8cmy"
      },
      "source": [
        "# Fairness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6_Vql4Z8cmy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Skip: Details\n",
        "\n",
        "def word_pattern(word: str) -> str:\n",
        "    \"\"\"\n",
        "    Create a pattern that matches the given `word` as long as it is not\n",
        "    immediately next to an alpha-numeric character.\n",
        "    \"\"\"\n",
        "    return \"(?<!\\w)\" + re.escape(word) + \"(?!\\w)\"\n",
        "\n",
        "\n",
        "def swap(thing1: str, thing2: str) -> Callable[[str], str]:\n",
        "    \"\"\"\n",
        "    Create a method to swap occurances of `thing1` and `thing2`.\n",
        "    \"\"\"\n",
        "\n",
        "    pat_swapper = re.compile(r\":swapper:\")\n",
        "    pat1 = re.compile(word_pattern(thing1), re.IGNORECASE)\n",
        "    pat2 = re.compile(word_pattern(thing2), re.IGNORECASE)\n",
        "\n",
        "    def f(sentence: str):\n",
        "        \"\"\"\n",
        "        Swap instances of thing1 and thing2 in sentence.\n",
        "        \"\"\"\n",
        "\n",
        "        temp1 = pat1.sub(\":swapper:\", sentence)\n",
        "        temp2 = pat2.sub(thing1, temp1)\n",
        "        temp3 = pat_swapper.sub(thing2, temp2)\n",
        "        return temp3\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def contains(s: str, pat: re.Pattern) -> bool:\n",
        "    \"\"\"\n",
        "    Determine whether the given string `s` satisfies regular expression `pat`.\n",
        "    \"\"\"\n",
        "    return pat.search(s) is not None\n",
        "\n",
        "\n",
        "def get_sentence_pairs(token_pairs: List[Tuple[str, str]],\n",
        "                       texts: List[str]) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Create sentence pairs from examples in `texts` that swap words from the\n",
        "    pairs list `token_pairs`.\n",
        "    \"\"\"\n",
        "\n",
        "    patterns = [\n",
        "        re.compile(\n",
        "            \"|\".join([word_pattern(tok) for tok in pair]), re.IGNORECASE\n",
        "        ) for pair in token_pairs\n",
        "    ]\n",
        "    swappers = [swap(*pair) for pair in token_pairs]\n",
        "\n",
        "    sentence_pairs = [\n",
        "        (sentence, swap(sentence))\n",
        "        for pattern, swap in\n",
        "        tqdm(zip(patterns, swappers), desc=\"finding swap pairs\", unit=\"pair\", leave=False)\n",
        "        for sentence in tqdm(texts, desc=\"processing sentences\", leave=False)\n",
        "        if contains(sentence, pattern)\n",
        "    ]\n",
        "\n",
        "    print(f\"found {len(sentence_pairs)} sentence pair(s)\")\n",
        "\n",
        "    return sentence_pairs\n",
        "\n",
        "# CLASS\n",
        "def compute_pair_disparities(\n",
        "    sentence_pairs: List[Tuple[str, str]]\n",
        ") -> List[Tuple[Tuple[str, str], float]]:\n",
        "    \"\"\"\n",
        "    Given a collection of `sentence_pairs`, produce a list of tuples containing\n",
        "    the pairs as the first element and the disparity in model scores as the\n",
        "    second.\n",
        "    \"\"\"\n",
        "\n",
        "    diffs = []\n",
        "    \n",
        "    # CLASS\n",
        "    a_probits = Model.evaluate_to_probits([pair[0] for pair in sentence_pairs])\n",
        "    b_probits = Model.evaluate_to_probits([pair[1] for pair in sentence_pairs])\n",
        "\n",
        "    # CLASS\n",
        "    for a_probit, b_probit in tqdm(zip(a_probits, b_probits),\n",
        "                                   desc=\"comparing probits\"):\n",
        "\n",
        "        diffs.append(\n",
        "            torch.nn.functional.cross_entropy(\n",
        "                torch.unsqueeze(a_probit, dim=0),\n",
        "                torch.unsqueeze(b_probit, dim=0)\n",
        "            ).detach().cpu().numpy()\n",
        "        )\n",
        "\n",
        "    diffs = np.array(diffs)\n",
        "    diffs_pairs = list(\n",
        "        reversed(sorted(zip(sentence_pairs, diffs), key=lambda pair: pair[1]))\n",
        "    )\n",
        "\n",
        "    return diffs_pairs\n",
        "\n",
        "\n",
        "def show_biggest_disparities(\n",
        "    diffs: List[Tuple[Tuple[str, str], float]],\n",
        "    attributor=infl_positive_baseline,\n",
        "    n=3\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Display the top disparate pairs along with their attributions.\n",
        "    \"\"\"\n",
        "\n",
        "    display(\n",
        "        V.tokens_stability(\n",
        "            texts1=[p[0][0] for p in diffs][0:n],\n",
        "            texts2=[p[0][1] for p in diffs][0:n],\n",
        "            attributor=attributor\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Robustness as fairness\n",
        "\n",
        "Does the model change its prediction if we replace one gendered word for its equivalent of the opposite gender?"
      ],
      "metadata": {
        "id": "tt3veAgz0leV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMsoRyJQ8cmy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Gendered pairs\n",
        "\n",
        "gender_pairs = [\n",
        "    ('he', 'she'),\n",
        "    ('guy', 'gal'),\n",
        "    ('himself', 'herself'),\n",
        "    ('boy', 'girl'),\n",
        "    ('husband', 'wife'),\n",
        "    ('man', 'woman'),\n",
        "    ('men', 'women'),\n",
        "    ('brother', 'sister'),\n",
        "    ('uncle', 'aunt'),\n",
        "    ('nephew', 'niece'),\n",
        "    ('dad', 'mom'),\n",
        "    ('father', 'mother'),\n",
        "    ('son', 'daughter'),\n",
        "    ('actor', 'actress'),\n",
        "    ('male', 'female'),\n",
        "    ('hero', 'heroine'),\n",
        "]\n",
        "\n",
        "sentence_pairs_gender = get_sentence_pairs(gender_pairs, rotten_texts)\n",
        "diffs_pairs_gender = compute_pair_disparities(sentence_pairs_gender)\n",
        "\n",
        "show_biggest_disparities(diffs_pairs_gender)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfTf1YeB8cmy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Playground: Fairness robustness under token substitutions\n",
        "#@markdown Enter two related tokens into the fields \"token1\" and \"token2\" to see which instances in the selected dataset differ the most in model outcomes between equivalent sentences that differ only in the two tokens. Some other examples to try:\n",
        "#@markdown - \"he\" vs \"she\"\n",
        "#@markdown - \"hero\" vs\" heroine\"\n",
        "#@markdown - ...\n",
        "\n",
        "@interact(\n",
        "    token1=textbox(\"hero\", d=\"token1\", c=False),\n",
        "    token2=textbox(\"heroine\", d=\"token2\", c=False),\n",
        "    dataset=widgets.Dropdown(\n",
        "        layout=aiq_layout,\n",
        "        options=[\n",
        "          (\"rotten tomatoes\", rotten_texts),\n",
        "          (\"imdb\", imdb_texts),\n",
        "          (\"tweet\", tweet_texts)\n",
        "        ], \n",
        "        style={'description_width': 'initial'}\n",
        "))\n",
        "def show_disparities(token1, token2, dataset):\n",
        "    if token1 == \"\" or token2 == \"\":\n",
        "        return\n",
        "\n",
        "    sentence_pairs = get_sentence_pairs([(token1, token2)], dataset)\n",
        "\n",
        "    if len(sentence_pairs) == 0:\n",
        "        return\n",
        "\n",
        "    diffs_pairs = compute_pair_disparities(sentence_pairs)\n",
        "    show_biggest_disparities(diffs_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOMP3Ka68cmz"
      },
      "source": [
        "## Gender in embedding space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZMW-xJJ8cmz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Skip: Details\n",
        "\n",
        "# A vector approximating the difference between embeddings of pairs of words\n",
        "# of the opposite gender. This one is for the token embedding used in\n",
        "# distilbert.\n",
        "\n",
        "# HUGS: Do not expect this to work with other models.\n",
        "gender_vector: npt.NDArray['float16'] = np.frombuffer(\n",
        "    base64.b85decode(\n",
        "        b'?4-;r+n_BbxT&BWWUIlV^Qc9YM<?$ffFvBOyeVWPZ6QZ1ttD(IWT3Vs@g>z1(IHALq#<!8`6&IFovN59FQQ<rjVfRxf2awiYAAOf0xHHLzn)JeL8GLalpxcndmiqk3oS&V=c3uGKP<ea=_dCgN-Vq~^C<SDBB=f;{-m#?lBazp6`Fi20ii4{HmQFaeJNI`eJG!zN2p7nT&MsSPpc;*^d-=!XDFVgZ7N3{HKO`0uqm#msif;Ew5=Wq#ib{lJ)>zVdn~jjwWg4$E-Hm9mL8ELf+nV@>7XN}EFh#J6Du&8d89uqq^KIJJfWAH<s(?A83iFILnMlrgei)r<*S^fJgHDAWT^b8{i%wqmaLPhbS9T6Jt#99Sf6L8+bydo;3dAJS}Uz4E1CW%<)@0Q!Ypv2O(|R<c%D?4xR(l{UL)hBP^^)syPV#omZOCyB&3O+Iv%o~8KW|tw5a_oU?h^MQ!4KuK_zc2qAUp_g(MRpzbQy4vZ$V-e<o?1A1Fqg-lZKX51h559ip|Y3@9lngeeCdV4_MV1tujcN-QR*NT-IaRil6?RICoHU?Y?#zN8MQpeo*^uBGuJ$tk-V*C>ytF{UOeM4I9z{VUa<Vx*-d-l$9`fGFf76r}JNuPLCYkTBb<^`&<w5-3!r4Wiwe2&9!9nI?EFcBnfjS1ipX@gcS+XD6R2W3AF1h>!iL@|kC$dla6YHl}MM<0kVa@~U*KA0pVH3?n6;I4Dsj{2DZ*bRJnH^CRFT$f!e}dniDvR;=tH3Mn0=KqeCw)1Lk)mZ)|kEUBrWAE-g9fhH>_i>J6EXBbkVJ|R9RW2H-=R-}(7FDB6^EvphMpez}nRGA2_{iNV0kf_8hG_1QR;-b?fwIDDpnI}W4-5<av4k0q42p)f_%Ay~t-K6*-@+Mjz%$YHy51(}(H7Uv>(<l=m@*IUHzoEq}BB-CJ^(USw-Yae=oTara=C19iiYMJ3?yV>$*qY=iC!+l-R;E;>I4FCe5iVFIf2tZC5UZpoNG!mnxS~QM38iYMm?*WNN1_!Zh^a27Eu`Qoc$}IfR+ikN7@WSNXQ}EcY!wqH2PuOl#HIx#l_0O7ogf&f@u_Slmm`>_AgeQ^RwvCWk}20BY9RBYb|sFgpeAH0Dx#e#tt9TASt-Dq@1$NOs3duoK`EM`FQsOske;`wT_Z0g0w<>+xh5K%eJIJG%cHs?)}7U(Vx_Yrcpk?fF(C#ZaHY4W)TXW`zn~3}NvZsnOQu36Q6W#L)T+R%yeKOyr=sMY+ou^QsHsz@^&6EdH6?hZOqr#q&8vec0VOpo7b`0ylcE<TLM8XAv7>h-WRQC!;i)z*1gXcS@g$NXP%J#EOdu8^39HJf-!4$5t1F|X>nB1bOs2G`_@1C8;w3n$h$<i`f+X;tZX^b#;Ucys8?I0(!zlPFE1h{F1FV`R&#Ig!^d}%BvmpE}lBB68#i@S|a3H`YHy+|3qoeSwl&35tv!2JIffi^f_$lxpN~Hd%My0>0I32|%;S;zZ38EJ*&ZM^~8z+q-g_sARQK)mI6e2np%BPPeTP$EE%&YOJg(wxF_94wE;wph611IgKq9+HaTdc1oA14&4ysA2+%p*K3yQG9HBPA3nuBm+}Bc<mgCL^*Z>ztA#$1O7^A1jL_&?9yoLLzG;4W}ldVyJc}@u_hmODydxda9wBL7j9dl?n!@W-8FCn5dm7l`5;HN2rgX&7LBtm8K*teWQw>D=p^^x~S!>GaR6#Mkv3kktndAR2qe)DyAc)iKy+Q&ZhH}K&En`@+qt+lOfouwj+$EbEZh08mM%hgeI${1Sv!*_$5}TS09xr!ljccx+a^awVhe0JfIpJSSmoK&7Kw@hbW;WB_D1h{;R}`OQ|d^%d6>`KqUt#-K4;xLadW0q^d!nF(UV!=_;$CJ1X3%@2E4Yk*i)Mz>y9k<fnnC5Gh|R<|O+jB#_&s2B9h)'\n",
        "    ),\n",
        "    dtype='float16'\n",
        ")\n",
        "\n",
        "\n",
        "def normalize(v: npt.NDArray[float]) -> npt.NDArray[float]:\n",
        "    \"\"\"\n",
        "    Normalize a single vector.\n",
        "    \"\"\"\n",
        "    return v / np.linalg.norm(v, ord=2)\n",
        "\n",
        "\n",
        "def normalize_many(v: npt.NDArray[float]) -> npt.NDArray[float]:\n",
        "    \"\"\"\n",
        "    Normalize an array of vectors.\n",
        "    \"\"\"\n",
        "    return v / np.linalg.norm(v, axis=1, ord=2)[:, np.newaxis]\n",
        "\n",
        "\n",
        "embeddings_norm = normalize_many(Model.embeddings)\n",
        "baseline_penalties = np.abs(np.dot(embeddings_norm, gender_vector))\n",
        "\n",
        "\n",
        "def embedding_opposite_id(emb: np.ndarray) -> Tuple[int, float]:\n",
        "    \"\"\"\n",
        "    Get the token id of the token closest to the gender-opposite of the given\n",
        "    `emb`.\n",
        "    \"\"\"\n",
        "\n",
        "    # HUGS\n",
        "    emb = normalize(emb)\n",
        "    scores = np.abs(\n",
        "        np.dot(\n",
        "            normalize_many(emb - embeddings_norm + 0.000000001), gender_vector\n",
        "        )\n",
        "    ) - 0.55 * baseline_penalties\n",
        "\n",
        "    best = np.argmax(scores)\n",
        "\n",
        "    return best, scores[best]\n",
        "\n",
        "\n",
        "def embedding_opposite(emb: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Try to find the embedding close to the opposite gender relative to the given\n",
        "    `emb`. \n",
        "    \"\"\"\n",
        "\n",
        "    best_id, best_score = embedding_opposite_id(emb)\n",
        "\n",
        "    # DATA\n",
        "    if best_score > 0.25:\n",
        "        return Model.embeddings[best_id]\n",
        "    else:\n",
        "        return emb\n",
        "\n",
        "\n",
        "def embedding_neutralize(emb: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Remove the component of the given embedding that points in the gender\n",
        "    direction.\n",
        "    \"\"\"\n",
        "    return emb - np.dot(emb, gender_vector) * gender_vector\n",
        "\n",
        "\n",
        "@functools.lru_cache(maxsize=Model.vocab_size)\n",
        "def token_id_opposite(token_id: int):\n",
        "    \"\"\"\n",
        "    Try to find the opposite of `token_id` according to the direction of\n",
        "    `direction_vector`. If a good candidate is not found, returns the given\n",
        "    `token_id` instead.\n",
        "    \"\"\"\n",
        "    best_id, best_score = embedding_opposite_id(embeddings_norm[token_id])\n",
        "\n",
        "    # DATA\n",
        "    if best_score > 0.20:\n",
        "        return best_id\n",
        "    else:\n",
        "        return token_id\n",
        "\n",
        "\n",
        "def swap_token(token: str) -> str:\n",
        "    \"\"\"\n",
        "    Attempts to find a token of the opposite gender of the given `token`.\n",
        "    \"\"\"\n",
        "\n",
        "    a_id = Model.id_of_token[token]\n",
        "    b_id = token_id_opposite(a_id)\n",
        "    return Model.token_of_id[b_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWh4ing48cmz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Playground: Gender in the embedding space\n",
        "#@markdown The embedding visualization we shown earlier in this notebook now includes indicators of the gender dimension with color. More blue tokens are more female while more red and male. \n",
        "#@markdown - You can use the sliders above the visualization to filter out tokens with small gender magnitude. The highlighted area between the two sliders indicates the gender magnitude range that is filtered out from the figure, so that only tokens with large magnitude are shown.\n",
        "#@markdown - Notice that if you drag both filter draggers to their end points, you end up with only the most female and most male tokens.\n",
        "#@markdown - Also note that there are clusters with similar gender despite that being only one of many aspects of a token embedding.\n",
        "#@markdown - On the other hand, there are clear deviations from the overall color/gender pattern. For example, the pairs \"man\"/\"men\" and \"woman\"/\"women\" are right next to each other despite featuring tokens of opposite genders. \n",
        "\n",
        "# geometry of gender in embedding space\n",
        "\n",
        "# AIQ: This is computationally intensive picture. It is only useful if you use\n",
        "# the tsne reduction.\n",
        "\n",
        "color = np.dot(normalize_many(Model.embeddings), gender_vector)\n",
        "cmin = color.min()\n",
        "cmax = color.max()\n",
        "  \n",
        "fig = Figure(layout=dict(width=800, height=800, **plotly_layout))\n",
        "fig.update_xaxes(\n",
        "    showticklabels=False\n",
        ")\n",
        "fig.update_yaxes(\n",
        "    showticklabels=False\n",
        ")\n",
        "s = fig.add_scatter(\n",
        "    x=[],\n",
        "    y=[],\n",
        "    text=Model.vocab,\n",
        "    mode='markers',\n",
        "    marker={\n",
        "        'cmin': cmin,\n",
        "        'cmax': cmax,\n",
        "        'colorscale': \"Picnic\",\n",
        "        'color': [],\n",
        "        'colorbar': dict(thickness=20)\n",
        "    },\n",
        "    marker_size=4\n",
        ")\n",
        "\n",
        "@interact(hide = widgets.FloatRangeSlider(continuous_update=False, value=[cmin/8, cmax/8], min=cmin, max=cmax, step=0.01, layout=aiq_layout))\n",
        "def show_gender_space(hide):\n",
        "\n",
        "  most_gendered = (color >= hide[1]) | (color <= hide[0])\n",
        "  s.data[0].update(\n",
        "    x=tsne_embedding[most_gendered, 0],\n",
        "    y=tsne_embedding[most_gendered, 1],\n",
        "    text=Model.vocab[most_gendered], \n",
        "    marker={\n",
        "      'color': color[most_gendered],\n",
        "    }\n",
        "  )\n",
        "\n",
        "  display(fig)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding debiasing"
      ],
      "metadata": {
        "id": "A6v1tYIH1S5c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DWx1E3u8cmz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Gender neutralized baseline definition\n",
        "\n",
        "def baseline_neutralize(z: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Given input tensor of embeddings, produce a baseline that removes their gender component. This can be used to debias words which you do not want to have a gender component like \"doctor\", \"nurse\", etc.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(z, torch.Tensor):\n",
        "        z = z.detach().cpu().numpy()\n",
        "\n",
        "    return torch.tensor(\n",
        "        np.array(\n",
        "            [[embedding_neutralize(emb) for emb in instance] for instance in z]\n",
        "        )\n",
        "    ).to(Model.device)\n",
        "\n",
        "\n",
        "infl_neutralize_gender = IntegratedGradients(\n",
        "    baseline=baseline_neutralize, **common_attributor_arguments\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gEpAKSy8cmz"
      },
      "outputs": [],
      "source": [
        "#@title Playground: Attribution to gender (original attribution on left, attribution to gender on right)\n",
        "#@markdown In this explanation variant, we have changed the baseline so that it only changes the gender component of gendered words. Words without clear gender components in their embedding are unchanged.\n",
        "#@markdown - Try changing the various pronounces in the example sentence one at a time and in various combinations.\n",
        "\n",
        "results3 = []\n",
        "\n",
        "\n",
        "@interact(\n",
        "    text=textbox(\n",
        "        t=\n",
        "        \"Johnson has, in his first film, set himself a task he is not nearly up to.\",\n",
        "        c=False\n",
        "    )\n",
        ")\n",
        "def show_attribution(text):\n",
        "    global results3\n",
        "\n",
        "    default_result = widgets.HTML(V.tokens([text], infl_positive_baseline).data)\n",
        "    baseline_result = widgets.HTML(\n",
        "        V.tokens([text], infl_neutralize_gender).data\n",
        "    )\n",
        "\n",
        "    results3.insert(0, (default_result, baseline_result))\n",
        "    results3 = results3[:3]\n",
        "\n",
        "    parts = []\n",
        "\n",
        "    for result in results3:\n",
        "        parts.append(widgets.HBox(result))\n",
        "\n",
        "    display(widgets.VBox(parts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0JNeCce8cmz"
      },
      "source": [
        "# Drift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_JVNrjK8cmz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title IMDB dataset\n",
        "\n",
        "# Get another dataset to compare to.\n",
        "\n",
        "# IMDB dataset is large, will take only a portion for speed:\n",
        "n = 2000\n",
        "\n",
        "# DATA: https://huggingface.co/datasets/imdb\n",
        "imdb_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-mvzEHPW1BI",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Dataset sampling and IMDB dataset accuracy\n",
        "\n",
        "def sample(items: List[Sequence], n: int) -> Tuple[List, List]:\n",
        "  \"\"\"\n",
        "  Take a sample of the sequences in `items`. If more than one sequence is given, the same indices are sampled from each, thus appropriate for X, Y pairs.\n",
        "  \"\"\"\n",
        "  if n > len(items[0]):\n",
        "    n = len(items[0])\n",
        "  items = list(map(np.array, items))\n",
        "  indices = np.random.choice(np.arange(len(items[0])), size=n).astype(int)\n",
        "  return tuple(map(lambda array: list(array[indices]), items))\n",
        "\n",
        "# DATA\n",
        "for dataset_name, (X, Ytrue) in [\n",
        "    (\"imdb train\", sample([imdb_train['text'], imdb_train['label']], n=n)),\n",
        "    (\"imdb test\", sample([imdb_test['text'], imdb_test['label']], n=n))]:\n",
        "    print(dataset_name, f\"accuracy = {accuracy(X, Ytrue) * 100:0.2f} %\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Skip: Details\n",
        "\n",
        "plotly_layout2 = plotly_layout.copy()\n",
        "plotly_layout2['margin'] = plotly_layout2['margin'].copy()\n",
        "plotly_layout2['margin']['t'] = 60\n",
        "\n",
        "# CLASS\n",
        "def show_model_score_drift(\n",
        "    texts1: List[str],\n",
        "    texts2: List[str],\n",
        "    n1: str,\n",
        "    n2: str,\n",
        "    score: str = \"positive\"\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Given two collections of texts, display model `score` histogram over those\n",
        "    two texts. The other arguments are for labeling the collections. \n",
        "    \"\"\"\n",
        "\n",
        "    # CLASS\n",
        "    scores1 = Model.evaluate_to_logits(texts1).detach().cpu().numpy()\n",
        "    scores2 = Model.evaluate_to_logits(texts2).detach().cpu().numpy()\n",
        "\n",
        "    # CLASS\n",
        "    df1 = pd.DataFrame(dict(\n",
        "        negative=scores1[:, 0],\n",
        "        positive=scores1[:, 1],\n",
        "    ))\n",
        "    df2 = pd.DataFrame(dict(\n",
        "        negative=scores2[:, 0],\n",
        "        positive=scores2[:, 1]\n",
        "    ))\n",
        "\n",
        "    s1 = df1[score]\n",
        "    s2 = df2[score]\n",
        "\n",
        "    counts1, bin_edges = np.histogram(s1, bins=20, density=True)\n",
        "    counts2, _ = np.histogram(s2, bins=bin_edges, density=True)\n",
        "\n",
        "    fig = Figure(layout=dict(title=\"model score distributions\", **plotly_layout2))\n",
        "    bar1 = fig.add_bar(x=bin_edges, y=counts1, name=n1)\n",
        "    bar2 = fig.add_bar(x=bin_edges, y=counts2, name=n2)\n",
        "\n",
        "    display(fig)"
      ],
      "metadata": {
        "id": "nw9J1j8H1wMy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YW3PHg188cmz"
      },
      "outputs": [],
      "source": [
        "#@title Playground: Model score drift (rotten tomatoes, train vs. test)\n",
        "#@markdown The distribution of scores of our sentiment model is shown in the histogram for two datasets: rotten tomatoes train split, and rotten tomatoes test split.\n",
        "\n",
        "# TODO: Can this visualization be adjusted to look more like the distribution plots in truera?\n",
        "\n",
        "# DATA\n",
        "show_model_score_drift(\n",
        "    sample([rotten_train['text']], n=n)[0],\n",
        "    sample([rotten_test['text']], n=n)[0],\n",
        "    'rotten train',\n",
        "    'rotten test'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3L3Ylrr8cmz"
      },
      "outputs": [],
      "source": [
        "#@title Playground: Model score drift (rotten tomatoes vs. imdb)\n",
        "#@markdown In this histogram, the score distributions are compared across two more significantly different datasets: rotten tomatoes and imdb.\n",
        "#@markdown - Compare the distributional differences visible here as compared to the previous plot.\n",
        "\n",
        "# DATA\n",
        "show_model_score_drift(\n",
        "    sample([rotten_train['text']], n=n)[0],\n",
        "    sample([imdb_train['text']], n=n)[0],\n",
        "    'rotten train',\n",
        "    'imdb train'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4_RIuHAkf4r"
      },
      "source": [
        "## Token distribution drift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYszoaB08cmz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Skip: Details\n",
        "\n",
        "def tokenize(portion: List[str]) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Tokenize into just token_ids, not any of the other model inputs.\n",
        "    \"\"\"\n",
        "    return Model.tokenizer.batch_encode_plus(\n",
        "        portion,\n",
        "        add_special_tokens=True,\n",
        "        return_attention_mask=False,\n",
        "        max_length=Model.max_length,\n",
        "        truncation=True\n",
        "    )['input_ids']\n",
        "\n",
        "\n",
        "def toks_of_texts(texts: List[str]) -> npt.NDArray[int]:    \n",
        "    toks = tokenize(texts)\n",
        "    \n",
        "    return np.array([t for tok in toks for t in tok])\n",
        "    \n",
        "\n",
        "def dists_of_texts(\n",
        "    texts: List[str]\n",
        ") -> Tuple[npt.NDArray[int], npt.NDArray[float]]:\n",
        "    all = toks_of_texts(texts)\n",
        "\n",
        "    counts = np.zeros(Model.vocab_size)\n",
        "\n",
        "    for i in all:\n",
        "        counts[i] += 1\n",
        "\n",
        "    dist = counts / len(all)\n",
        "\n",
        "    return counts, dist\n",
        "\n",
        "\n",
        "def tops_of_texts(texts: List[str], n: int = 10) -> List[int]:\n",
        "    \"\"\"\n",
        "    Get the indices of the most frequent tokens in the collection of `texts`.\n",
        "    \"\"\"\n",
        "\n",
        "    counts, dist = dists_of_texts(texts)\n",
        "\n",
        "    return tops_of_dists(counts, dist, n=n)\n",
        "\n",
        "\n",
        "def tops_of_dists(c: npt.NDArray[int],\n",
        "                  d: npt.NDArray[float],\n",
        "                  n=10) -> List[int]:\n",
        "    sortindex = np.argsort(d)\n",
        "    top = []\n",
        "\n",
        "    for idx in sortindex[0:n]:\n",
        "        top.append((idx, c[idx], d[idx], Model.tokenizer.decode(idx)))\n",
        "\n",
        "    crest_pos = 0\n",
        "    crest_neg = 0\n",
        "    drest_pos = 0\n",
        "    drest_neg = 0\n",
        "\n",
        "    for idx in sortindex[n:-n]:\n",
        "        if c[idx] >= 0:\n",
        "            crest_pos += c[idx]\n",
        "            drest_pos += d[idx]\n",
        "        else:\n",
        "            crest_neg += c[idx]\n",
        "            drest_neg += d[idx]\n",
        "\n",
        "    top.append((-1, crest_neg, drest_neg, \"*\"))\n",
        "    top.append((-1, crest_pos, drest_pos, \"*\"))\n",
        "\n",
        "    for idx in sortindex[-n:]:\n",
        "        top.append((idx, c[idx], d[idx], Model.tokenizer.decode(idx)))\n",
        "\n",
        "    return top\n",
        "\n",
        "def plotdist(\n",
        "    d1: npt.NDArray[float], d2: npt.NDArray[float], top, l1: str, l2: str\n",
        ") -> None:\n",
        "\n",
        "    n = len(top)\n",
        "\n",
        "    dprobs = pd.DataFrame(\n",
        "        {\n",
        "            \"token\": [t[3] for t in top] * 2,\n",
        "            \"dataset\": ([l1] * n) + ([l2] * n),\n",
        "            \"prob\": [d1[t[0]] for t in top] + [d2[t[0]] for t in top]\n",
        "        }\n",
        "    )\n",
        "    fig = px.bar(dprobs, x=\"token\", y=\"prob\", color=\"dataset\", barmode='group')\n",
        "    fig.update_layout(plotly_layout, height=300)\n",
        "    display(fig)\n",
        "\n",
        "    ddiff = pd.DataFrame(\n",
        "        {\n",
        "            \"token\": [t[3] for t in top],\n",
        "            \"prob\": [t[2] for t in top]\n",
        "        }\n",
        "    )\n",
        "    fig = px.bar(ddiff, x=\"token\", y=\"prob\")\n",
        "    fig.update_layout(plotly_layout, height=300)\n",
        "    display(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leejbanU8cmz"
      },
      "outputs": [],
      "source": [
        "#@title Playground: Token distribution drift (imdb vs rotten tomatoes)\n",
        "#@markdown In this distributional comparison, we look not at model scores, but token frequencies in the data (hence this is model-independent analysis which may or may not be consequential to the model).\n",
        "#@markdown - Notice that many of the distributional differences are due to inclusion or exclusion of HTML tags in the datasets.\n",
        "#@markdown - The \"*\" on the token axis represents all other tokens (the probabilities are ommitted on the upper chart, but total differences are shown in the lower chart).\n",
        "\n",
        "# DATA\n",
        "c1, d1 = dists_of_texts(imdb_train['text'][:n])\n",
        "c2, d2 = dists_of_texts(rotten_train['text'][:n])\n",
        "top = tops_of_dists(c1 - c2, d1 - d2, n=20)\n",
        "\n",
        "# DATA\n",
        "plotdist(d1, d2, top, l1='imdb', l2='rotten')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRSHyeg_8cmz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Playground: Token distribution drift (imdb train vs. imdb test)\n",
        "#@markdown Here we are comparing the two splits of the rotten tomatoes data.\n",
        "#@markdown - Note that while the total difference in token probabilities adds to a significant amount, no single token offers much difference.\n",
        "\n",
        "# DATA\n",
        "c1, d1 = dists_of_texts(imdb_train['text'][:n])\n",
        "c2, d2 = dists_of_texts(imdb_test['text'][:n])\n",
        "top = tops_of_dists(c1 - c2, d1 - d2, n=20)\n",
        "\n",
        "# DATA\n",
        "plotdist(d1=d1, d2=d2, top=top, l1='imdb train', l2='imdb test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2oHZBJt8cmz"
      },
      "source": [
        "## Embedding distribution drift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlZIYs-d8cm0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Skip: Details\n",
        "\n",
        "# DATA\n",
        "c1, d1 = dists_of_texts(rotten_train['text'][:n])\n",
        "c2, d2 = dists_of_texts(imdb_train['text'][:n])\n",
        "\n",
        "data1 = dict(prob=d1, token_id=range(Model.vocab_size))\n",
        "data1.update({f\"dim{did}\": Model.embeddings[:, did] for did in range(Model.embedding_size)})\n",
        "df1 = pd.DataFrame(data1)\n",
        "\n",
        "data2 = dict(prob=d2, token_id=range(Model.vocab_size))\n",
        "data2.update({f\"dim{did}\": Model.embeddings[:, did] for did in range(Model.embedding_size)})\n",
        "df2 = pd.DataFrame(data2)\n",
        "\n",
        "def show_hists(\n",
        "    s1: pd.Series, s2: pd.Series, df1: pd.DataFrame, df2: pd.DataFrame,\n",
        "    title: str\n",
        ") -> None:\n",
        "    counts1, bin_edges = np.histogram(s1, bins=20, weights=df1.prob.values)\n",
        "    counts2, _ = np.histogram(s2, bins=bin_edges, weights=df2.prob.values)\n",
        "\n",
        "    fig = go.Figure(layout=dict(title=title, **plotly_layout2))\n",
        "    fig.update_layout(xaxis_title=\"Dimension's value\", yaxis_title=\"Density\")\n",
        "    # DATA\n",
        "    bar1 = fig.add_bar(x=bin_edges, y=counts1, name=\"rotten\")\n",
        "    bar2 = fig.add_bar(x=bin_edges, y=counts2, name=\"imdb\")\n",
        "\n",
        "    display(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_pKegDP8cm0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Playground: Embedding distribution shift\n",
        "#@markdown In this distributions plot we visualize the relative occurace frequencies of the token embeddings, one dimension at a time.\n",
        "#@markdown - The slider on top of the graph lets you change which of the many embedding dimensions to focus on.\n",
        "\n",
        "@interact(dim=widgets.IntSlider(value=0, min=0, max=Model.embedding_size-1, layout=aiq_layout))\n",
        "def show_dim_hist(dim):\n",
        "    show_hists(\n",
        "        df1[f'dim{dim}'],\n",
        "        df2[f'dim{dim}'],\n",
        "        df1,\n",
        "        df2,\n",
        "        title=f\"embedding dimension {dim}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPBWRuw78cm0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Playground: Gender distribution drift\n",
        "#@markdown While the distributional differences in raw embedding dimensions may not be interpretable, in this figure we instead compare the distributions of the gender direction of the tokens in each dataset.\n",
        "#@markdown - In this setting, a significant difference can also be associated with an interpretation that reads something like: \"the overall gender of the dataset has shifted towards more female\".\n",
        "\n",
        "embeddings_gender = np.dot(Model.embeddings, gender_vector)\n",
        "\n",
        "df1g = pd.DataFrame(dict(\n",
        "    gender=embeddings_gender,\n",
        "    prob=d1,\n",
        "    token_id=range(Model.vocab_size)\n",
        "))\n",
        "\n",
        "df2g = pd.DataFrame(dict(\n",
        "    gender=embeddings_gender,\n",
        "    prob=d2,\n",
        "    token_id=range(Model.vocab_size)\n",
        "))\n",
        "\n",
        "show_hists(\n",
        "    df1g.gender, df2g.gender, df1g, df2g, title=\"gender dimension histogram\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [
        "U_gvi71u8cmx",
        "pfPwNFdG8cmx"
      ],
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.13 ('python37_pytorch_cuda')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "5c887fbf053ca26987bbc53439e0f6b5aed35740853e2f251f93c3825002a8ec"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}